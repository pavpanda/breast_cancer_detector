{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Author: Pavan Pandurangi\n",
    "# Last Modified: 7/29/19\n",
    "# This program roughly sketches a three layer neural network (30-30-1) that\n",
    "# attempts to classify a breast cancer tumor as malignant or benign based on 30 features.\n",
    "# The neural network uses sigmoid as its activation function. Theta is the variable\n",
    "# denoted for weights and lambda (lambd) is used for regularization. X is commonly used\n",
    "# to denote features while y is commonly used to denote the labels. z is used to denote\n",
    "# values of a layer in the pre-activiation stage, while a is used to denote values of a \n",
    "# layer in the post-activation stage (other than the input layer).\n",
    "\n",
    "# note: this roughly created ANN does not have an implementation\n",
    "# using cross-validation data yet. This will be incorporated later.\n",
    "# For now, the values of the parameters have been chosen by hand.\n",
    "\n",
    "# this function creates a dataframe using the data.csv file\n",
    "# which contains the breast cancer training and testing data.\n",
    "# It then gets rid of unwanted columns (unnecessary for implementation)\n",
    "# and separates the dataset into training and testing data (random examples\n",
    "# in both datasets)\n",
    "def clean_data(csv_file_name, training_percent, testing_percent):\n",
    "    data_set = pd.read_csv(csv_file_name)\n",
    "    \n",
    "    data_set[\"diagnosis\"] = data_set[\"diagnosis\"].replace(\"M\", 1) # Malignant is 1\n",
    "    data_set[\"diagnosis\"] = data_set[\"diagnosis\"].replace(\"B\", 0) # Benign is 0\n",
    "    \n",
    "    data_set = data_set.drop([\"id\", \"Unnamed: 32\"], axis = 1)\n",
    "    \n",
    "    data_set = (data_set - data_set.min()) / (data_set.max() - data_set.min())\n",
    "    \n",
    "    data_set = data_set.sample(frac=1)\n",
    "    \n",
    "    training_set = data_set.head(int(len(data_set)*(training_percent / 100)))\n",
    "    testing_set = data_set.tail(int(len(data_set)*(testing_percent / 100)))\n",
    "\n",
    "    return training_set, testing_set\n",
    "\n",
    "# this function is the core function for training the ANN (it calculates the proper\n",
    "# weights that correlate the training features with the labels). First, the design\n",
    "# (feature) matrix, X, is created along with the testing feature matrix. The training\n",
    "# and testing label vectors are created as well. The weight matrices, Theta1 and Theta2,\n",
    "# are created, where Theta1 represents the weights applied from the input layer to the\n",
    "# hidden layer, and Theta2 represents the weights applied from the hidden layer to the\n",
    "# output layer. Gradient descent is then run to calculate optimal values for Theta1 and Theta2\n",
    "# based on the training set. The testing set is used to calculate the accuracy of the algorithm.\n",
    "def train_algorithm(training_set, testing_set, num_iters, learning_rate, lambd):\n",
    "    # define dataset features and label\n",
    "    X = np.array(training_set.drop([\"diagnosis\"], axis = 1)) # 569 x 30\n",
    "    X = np.hstack((np.ones((len(X), 1)), X)) # now 569 x 31\n",
    "    X_test = np.array(testing_set.drop([\"diagnosis\"], axis = 1))\n",
    "    X_test = np.hstack((np.ones((len(X_test), 1)), X_test))\n",
    "    y = np.reshape(np.array(training_set[\"diagnosis\"]), (-1, 1)) # 569 x 1\n",
    "    y_test = np.reshape(np.array(testing_set[\"diagnosis\"]), (-1, 1)) # 569 x 1\n",
    "\n",
    "    INIT_EPSILON = 0.001\n",
    "    Theta1 = np.random.rand(30, 31) * 2 * INIT_EPSILON - INIT_EPSILON\n",
    "    Theta2 = np.random.rand(1, 31) * 2 * INIT_EPSILON - INIT_EPSILON\n",
    "    \n",
    "    Theta1, Theta2 = perform_gradient_descent(X, y, num_iters, Theta1, Theta2, learning_rate, lambd)\n",
    "    \n",
    "    hypothesis, accuracy = feedforward(X_test, y_test, Theta1, Theta2, True)\n",
    "    return hypothesis, accuracy\n",
    "    \n",
    "# this function runs the gradient descient algorithm with the training set X with num_iters\n",
    "# iterations. The starting values of the weights matrices are specified as init_theta1\n",
    "# and init_theta2. The learning rate is specified and the regularization parameter lambda (lambd) is\n",
    "# also specified.\n",
    "def perform_gradient_descent(X, y, num_iters, init_theta1, init_theta2, learning_rate, lambd):\n",
    "    theta1 = init_theta1\n",
    "    theta2 = init_theta2\n",
    "    for i in range(num_iters):\n",
    "        theta1_grad, theta2_grad = getCostGradients(X, y, theta1, theta2, lambd)\n",
    "        temp1 = theta1\n",
    "        temp2 = theta2\n",
    "        \n",
    "        temp1 = temp1 - learning_rate * theta1_grad\n",
    "        temp2 = temp2 - learning_rate * theta2_grad\n",
    "        \n",
    "        theta1 = temp1\n",
    "        theta2 = temp2\n",
    "    return theta1, theta2\n",
    "\n",
    "# this function calculates the gradients needed for each iteration of\n",
    "# gradient descent by feeding foward through the neural network and \n",
    "# backpropagating.\n",
    "def getCostGradients(X, y, theta1, theta2, lambd):\n",
    "    a1, z2, a2, z3, a3 = feedforward(X, y, theta1, theta2, False)\n",
    "    return backprop(y, a3, z2, a2, a1, theta1, theta2, lambd)\n",
    "\n",
    "# this function uses an input X and calculates the hypothesis (prediction)\n",
    "# utilizing the weights. If the user is testing the accuracy of the algorithm,\n",
    "# the accuracy is returned along with the prediction.\n",
    "def feedforward(X, y, theta1, theta2, testing):\n",
    "    a1 = X # a1: 569 x 31\n",
    "    z2 = np.matmul(a1, theta1.T) # 569 x 30\n",
    "    a2 = np.hstack((np.ones((len(z2), 1)), sigmoid(z2))) # 569 x 31\n",
    "    z3 = np.matmul(a2, theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    if not testing:\n",
    "        return a1, z2, a2, z3, a3\n",
    "    else:\n",
    "        costs = np.round(a3) - y\n",
    "        count = 0\n",
    "        for i in costs:\n",
    "            if i == 0:\n",
    "                count += 1\n",
    "        accuracy = (count / len(X))\n",
    "        return np.round(a3), accuracy\n",
    "\n",
    "# this function backpropagates through the neural network starting\n",
    "# at the output layer and calculates the gradients for each weight\n",
    "# matrix. The bias elements are dealt with to allow for the vectorized\n",
    "# calculations to run without error.\n",
    "def backprop(y, a3, z2, a2, a1, theta1, theta2, lambd):\n",
    "    m = len(y)\n",
    "    d3 = a3 - y\n",
    "    \n",
    "    non_bias_theta2 = np.delete(theta2, (0), axis = 1)\n",
    "    \n",
    "    d2 = np.matmul(d3, non_bias_theta2) * sigmoid_gradient(z2)\n",
    "    \n",
    "    delta1 = np.matmul(d2.T, a1)\n",
    "    delta2 = np.matmul(d3.T, a2)\n",
    "    \n",
    "    non_bias_theta1 = np.delete(theta1, (0), axis = 1)\n",
    "\n",
    "    zero_bias_theta1 = np.hstack((np.zeros((len(non_bias_theta1), 1)), non_bias_theta1))\n",
    "    zero_bias_theta2 = np.hstack((np.zeros((len(non_bias_theta2), 1)), non_bias_theta2))\n",
    "    \n",
    "    p1 = lambd * zero_bias_theta1\n",
    "    p2 = lambd * zero_bias_theta2\n",
    "    \n",
    "    theta1_grad = (1 / m) * (delta1 + p1)\n",
    "    theta2_grad = (1 / m) * (delta2 + p2)\n",
    "    return theta1_grad, theta2_grad\n",
    "\n",
    "# this function is the activation function used for each layer of the neural network\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.e ** (-z))\n",
    "\n",
    "# this function is the gradient of the activation function and is needed during\n",
    "# backpropagation to calculate the weight gradients.\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "0.9764705882352941\n"
     ]
    }
   ],
   "source": [
    "# using a 70 / 30 split, will probably change to 60 / 20 / 20 with cross-validation.\n",
    "training_set, testing_set = clean_data(\"data.csv\", 70, 30)\n",
    "hypothesis, accuracy = train_algorithm(training_set, testing_set, 10000, 0.1, 0.5)\n",
    "print(hypothesis)\n",
    "print(accuracy) # 97.0% - 98.88% accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
